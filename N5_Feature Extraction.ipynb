{"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","The original dataset have 3406 features (wavenumber), in which all features are not important and highly correlated to each other. Hence, feature extraction is used to extract the features. This code file is for feature extraction."],"metadata":{"id":"VubFKL5WnvuK"},"id":"VubFKL5WnvuK"},{"cell_type":"markdown","source":["## Loading Libraries"],"metadata":{"id":"Tm81hAyLn6vf"},"id":"Tm81hAyLn6vf"},{"cell_type":"code","execution_count":null,"id":"6e845c55","metadata":{"id":"6e845c55"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.decomposition import PCA\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.preprocessing import StandardScaler, scale"]},{"cell_type":"code","execution_count":null,"id":"a1132c14","metadata":{"id":"a1132c14"},"outputs":[],"source":["plt.rcParams['font.family'] = 'Times New Roman'"]},{"cell_type":"markdown","source":["## Loading Dataset\n","\n","To run other dataset: Just change Dataset P1, P2, ..., OFE, PFE."],"metadata":{"id":"7qdIxX97n8Im"},"id":"7qdIxX97n8Im"},{"cell_type":"code","execution_count":null,"id":"4d967bcb","metadata":{"id":"4d967bcb"},"outputs":[],"source":["filename='Dataset O.xlsx'\n","df=pd.read_excel(filename, index_col=0)\n","df.head()"]},{"cell_type":"code","execution_count":null,"id":"88228918","metadata":{"id":"88228918"},"outputs":[],"source":["X, Y = df.iloc[:, :-1], df.iloc[:, -1]\n","\n","stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n","for train_index, test_index in stratified_split.split(X, Y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n","\n","print('X_train: {}     Y_train: {} \\nX_test: {}     Y_test: {}'.format(X_train.shape, Y_train.shape,\n","                                                                       X_test.shape, Y_test.shape))"]},{"cell_type":"markdown","source":["## Feature Extraction"],"metadata":{"id":"k4n1Knk0n_vm"},"id":"k4n1Knk0n_vm"},{"cell_type":"markdown","source":["### 1. PCA\n","\n","Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and data analysis to transform a high-dimensional dataset into a lower-dimensional space while preserving as much variance (information) as possible."],"metadata":{"id":"RN33P6KkoAz1"},"id":"RN33P6KkoAz1"},{"cell_type":"code","execution_count":null,"id":"01a732b9","metadata":{"id":"01a732b9"},"outputs":[],"source":["df.columns=df.columns.astype(str)"]},{"cell_type":"code","execution_count":null,"id":"ef9811c2","metadata":{"id":"ef9811c2"},"outputs":[],"source":["scaler = StandardScaler()\n","standardized_data = scaler.fit_transform(df)"]},{"cell_type":"code","execution_count":null,"id":"1f3d70d8","metadata":{"id":"1f3d70d8"},"outputs":[],"source":["pca = PCA(n_components=None)\n","principal_components = pca.fit_transform(standardized_data)"]},{"cell_type":"code","execution_count":null,"id":"f74fc04c","metadata":{"id":"f74fc04c"},"outputs":[],"source":["# Scree Plot\n","explained_variance_ratio_cumsum = np.cumsum(pca.explained_variance_ratio_)\n","plt.plot(explained_variance_ratio_cumsum, label='Cumulative Explained Variance')\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Cumulative Explained Variance')\n","#plt.title('Scree Plot')\n","plt.grid(True)\n","plt.legend()\n","\n","# Save the figure with 600 DPI as a JPEG image\n","plt.savefig('F3.jpg', dpi=600, format='jpg')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"b7471977","metadata":{"id":"b7471977"},"outputs":[],"source":["# Set a threshold for cumulative explained variance (e.g., 95%)\n","cumulative_variance_threshold = 0.95\n","\n","# Determine the optimal number of components based on the threshold\n","n_optimal_components = np.argmax(explained_variance_ratio_cumsum >= cumulative_variance_threshold) + 1"]},{"cell_type":"code","execution_count":null,"id":"13b1a600","metadata":{"id":"13b1a600"},"outputs":[],"source":["print(n_optimal_components)"]},{"cell_type":"code","execution_count":null,"id":"d8180a1d","metadata":{"id":"d8180a1d"},"outputs":[],"source":["print(f'Number of Principal Components to Explain {cumulative_variance_threshold * 100}% Variance: {n_optimal_components}')"]},{"cell_type":"code","execution_count":null,"id":"0ed66984","metadata":{"id":"0ed66984"},"outputs":[],"source":["# Combine principal components and target variable into a single DataFrame\n","pc_df = pd.DataFrame(data=principal_components[:, :n_optimal_components],\n","                     columns=[f'PC{i}' for i in range(1, n_optimal_components + 1)])\n","pc_df['Target'] = Y\n","\n","# Create regression plots for each principal component\n","plt.figure(figsize=(10, 6))\n","for i in range(n_optimal_components):\n","    plt.subplot(1, n_optimal_components, i + 1)\n","    sns.regplot(x=f'PC{i+1}', y='Target', data=pc_df, scatter_kws={'alpha':0.5})\n","    #plt.title(f'PC{i+1} vs. Target')\n","    plt.xlabel(f'PC{i+1}')\n","    plt.ylabel('Target')\n","\n","plt.tight_layout()\n","\n","# Save the figure with 600 DPI as a JPEG image\n","plt.savefig('F4.jpg', dpi=600, format='jpg')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"8cc82e85","metadata":{"id":"8cc82e85"},"outputs":[],"source":["# Variance Explained Bar Plot\n","plt.bar(range(1, n_optimal_components + 1), pca.explained_variance_ratio_[:n_optimal_components],\n","        alpha=0.8, align='center', label='Individual Explained Variance')\n","plt.step(range(1, n_optimal_components + 1), explained_variance_ratio_cumsum[:n_optimal_components],\n","         where='mid', label='Cumulative Explained Variance', color='red')\n","plt.xlabel('Number of Principal Components')\n","plt.ylabel('Explained Variance Ratio')\n","#plt.title('Variance Explained Bar Plot (PCA)')\n","plt.legend(loc='center right')\n","\n","# Save the figure with 600 DPI as a JPEG image\n","plt.savefig('F5.jpg', dpi=600, format='jpg')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"46d45938","metadata":{"id":"46d45938"},"outputs":[],"source":["# Create a DataFrame with all optimal principal components\n","df_ofe = pd.DataFrame(principal_components[:, :n_optimal_components],\n","                      columns=[f'PC{i}' for i in range(1, n_optimal_components + 1)])\n","\n","# Include the 'Target' column\n","df_ofe['Target'] = df['Target']\n","\n","# Display the updated dataframe\n","df_ofe.head()"]},{"cell_type":"code","execution_count":null,"id":"4916df0c","metadata":{"id":"4916df0c"},"outputs":[],"source":["df_ofe.to_excel('Dataset OFE.xlsx')"]},{"cell_type":"markdown","source":["### 2. LDA\n","\n","Linear Discriminant Analysis (LDA) is a supervised dimensionality reduction and classification technique used in machine learning to find linear combinations of features that best separate classes in a dataset."],"metadata":{"id":"Gi_XKRIhoLI2"},"id":"Gi_XKRIhoLI2"},{"cell_type":"code","execution_count":null,"id":"8a608ecd","metadata":{"id":"8a608ecd"},"outputs":[],"source":["# Set a threshold for binary classification (e.g., 0.5)\n","threshold = 0.5\n","\n","# Convert 'Target' variable to binary labels\n","df['Target'] = (df['Target'] > threshold).astype(int)\n","\n","# Check the unique values in the 'Target' variable after conversion\n","print(\"Unique values in 'Target' variable:\", df['Target'].unique())\n","\n","# Now, you can apply LDA\n","lda = LDA(n_components=None)\n","lda_components = lda.fit_transform(standardized_data, df['Target'])"]},{"cell_type":"code","execution_count":null,"id":"3bd73b27","metadata":{"scrolled":true,"id":"3bd73b27"},"outputs":[],"source":["# LDA Scatter Plot\n","plt.figure(figsize=(8, 6))\n","\n","# Scatter plot for class 0\n","plt.scatter(lda_components[df['Target'] == 0], np.zeros_like(lda_components[df['Target'] == 0]),\n","            label='Class 0', alpha=0.8)\n","\n","# Scatter plot for class 1\n","plt.scatter(lda_components[df['Target'] == 1], np.zeros_like(lda_components[df['Target'] == 1]),\n","            label='Class 1', alpha=0.8)\n","\n","plt.xlabel('Linear Discriminant 1')\n","plt.title('LDA Scatter Plot')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"c4f1c7a8","metadata":{"id":"c4f1c7a8"},"outputs":[],"source":["# Set a threshold for binary classification (e.g., 0.5)\n","threshold = 0.5\n","\n","# Convert 'Target' variable to binary labels\n","df['Binary_Target'] = (df['Target'] > threshold).astype(int)\n","\n","# Check the unique values in the 'Binary_Target' variable after conversion\n","print(\"Unique values in 'Binary_Target' variable:\", df['Binary_Target'].unique())"]},{"cell_type":"code","execution_count":null,"id":"bba3fab4","metadata":{"scrolled":true,"id":"bba3fab4"},"outputs":[],"source":["# Now, you can apply LDA\n","lda = LDA(n_components=None)\n","lda_components = lda.fit_transform(standardized_data, df['Target'])\n","\n","# Number of LDA components\n","n_lda_components = lda_components.shape[1]\n","\n","# Create a DataFrame with all LDA components\n","df_lda = pd.DataFrame(lda_components, columns=[f'LDA{i}' for i in range(1, n_lda_components + 1)])"]},{"cell_type":"code","execution_count":null,"id":"800a26f2","metadata":{"id":"800a26f2"},"outputs":[],"source":["# Include the 'Target' column\n","df_lda['Target'] = df['Target']"]},{"cell_type":"code","execution_count":null,"id":"19002506","metadata":{"id":"19002506"},"outputs":[],"source":["# Display the updated dataframe for LDA\n","print(\"\\nLDA Features:\")\n","print(df_lda.head())"]},{"cell_type":"code","execution_count":null,"id":"e272ce9b","metadata":{"id":"e272ce9b"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}