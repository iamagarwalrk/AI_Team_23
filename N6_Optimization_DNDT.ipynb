{"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","As our decision tree have also acceptable result, we also optimized the deep neural decision tree for this dataset.\n","\n","A Deep Neural Decision Tree (DNDT) is a hybrid machine learning model that combines the interpretability of decision trees with the expressive power of deep neural networks. It leverages neural networks to mimic the hierarchical decision-making process of a decision tree, where decisions at each node are learned in a differentiable, end-to-end manner."],"metadata":{"id":"FjfqHmjlphMn"},"id":"FjfqHmjlphMn"},{"cell_type":"markdown","source":["## Loading Libraries"],"metadata":{"id":"UzBynnk5psIt"},"id":"UzBynnk5psIt"},{"cell_type":"code","execution_count":null,"id":"947686d3","metadata":{"id":"947686d3"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import os\n","from sklearn.model_selection import KFold, StratifiedShuffleSplit\n","from sklearn.feature_selection import SelectKBest\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Layer\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.constraints import Constraint\n","from bayes_opt import BayesianOptimization"]},{"cell_type":"code","execution_count":null,"id":"5bf35915","metadata":{"id":"5bf35915"},"outputs":[],"source":["plt.rcParams['font.family'] = 'Times New Roman'"]},{"cell_type":"markdown","source":["# constraint"],"metadata":{"id":"uHpUjs99p0nO"},"id":"uHpUjs99p0nO"},{"cell_type":"code","execution_count":null,"id":"0615bc9c-ae9a-491e-a31f-06c4654035ee","metadata":{"id":"0615bc9c-ae9a-491e-a31f-06c4654035ee"},"outputs":[],"source":["class SetConstraint(Constraint):\n","    def __init__(self, min_val=-10, max_val=10): # constraint weight range\n","        self.min_val = min_val\n","        self.max_val = max_val\n","\n","    def __call__(self, w):\n","        return tf.clip_by_value(w, self.min_val, self.max_val)\n","\n","    def get_config(self):\n","        return {'min_val': self.min_val, 'max_val': self.max_val}"]},{"cell_type":"code","execution_count":null,"id":"95798674","metadata":{"id":"95798674"},"outputs":[],"source":["# Loading Dataset"]},{"cell_type":"code","execution_count":null,"id":"bb845544","metadata":{"id":"bb845544"},"outputs":[],"source":["filename = 'Dataset O.xlsx'\n","df = pd.read_excel(filename, index_col=0)\n","X, Y = df.iloc[:, :-1], df.iloc[:, -1]"]},{"cell_type":"code","execution_count":null,"id":"34a8ba63-a1d0-4c84-adbb-df2f974e912e","metadata":{"id":"34a8ba63-a1d0-4c84-adbb-df2f974e912e"},"outputs":[],"source":["# Feature selection"]},{"cell_type":"code","execution_count":null,"id":"ae4a61c9-9601-4146-a8c9-b11b3a6c5c24","metadata":{"id":"ae4a61c9-9601-4146-a8c9-b11b3a6c5c24"},"outputs":[],"source":["selector = SelectKBest(k=10)\n","X = selector.fit_transform(X, Y)"]},{"cell_type":"code","execution_count":null,"id":"c82a85fd-3723-4318-af16-83e92cd097e3","metadata":{"id":"c82a85fd-3723-4318-af16-83e92cd097e3"},"outputs":[],"source":["stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=42)\n","\n","for train_index, test_index in stratified_split.split(X, Y):\n","    X_train, X_test = X[train_index], X[test_index]\n","    Y_train, Y_test = Y[train_index], Y[test_index]\n","\n","print('X_train: {}     Y_train: {} \\n X_test: {}     Y_test:{} '.format(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape))"]},{"cell_type":"code","execution_count":null,"id":"c30fc760-6807-46af-99a0-004687b2c139","metadata":{"id":"c30fc760-6807-46af-99a0-004687b2c139"},"outputs":[],"source":["class DNDTLayer(Layer):\n","    def __init__(self, n_cuts=1, temperature=10.0, **kwargs):\n","        super(DNDTLayer, self).__init__(**kwargs)\n","        self.n_cuts = n_cuts\n","        self.temperature = temperature\n","        self.set_constraint = SetConstraint()\n","\n","    def build(self, input_shape):\n","        n_features = input_shape[-1]\n","\n","        # init cut points for features\n","        self.cut_points = self.add_weight(\n","            name='cut_points',\n","            shape=(n_features, self.n_cuts),\n","            initializer='uniform',\n","            trainable=True,\n","            constraint=self.set_constraint\n","        )\n","        # init leaf scores\n","        self.n_leaves = (self.n_cuts + 1) ** n_features\n","        self.leaf_scores = self.add_weight(\n","            name='leaf_scores',\n","            shape=(self.n_leaves, 1),\n","            initializer='zeros',\n","            trainable=True,\n","            constraint=self.set_constraint\n","        )\n","\n","    def call(self, inputs):\n","        # soft binning\n","        diffs = tf.expand_dims(inputs, axis=-1) - self.cut_points # [batch, features, cuts]\n","        bin_probs = tf.sigmoid(diffs * self.temperature)\n","\n","        # get leaf probabilities\n","        leaf_probs = tf.ones([tf.shape(inputs)[0], 1])  # Initialize with ones\n","        for i in range(inputs.shape[-1]):\n","            # Get probabilities for each feature's bins\n","            feature_probs = tf.concat([\n","                tf.reduce_prod(1 - bin_probs[:, i, :], axis=1, keepdims=True),  # Leftmost bin\n","                bin_probs[:, i, :-1] * tf.reduce_prod(1 - bin_probs[:, i, 1:], axis=1, keepdims=True),  # Middle bins\n","                tf.reduce_prod(bin_probs[:, i, :], axis=1, keepdims=True)  # Rightmost bin\n","            ], axis=1)\n","\n","            # Update\n","            leaf_probs = tf.expand_dims(leaf_probs, axis=-1) * tf.expand_dims(feature_probs, axis=1)\n","            leaf_probs = tf.reshape(leaf_probs, [-1, (self.n_cuts+1)**(i+1)])\n","\n","        # predict\n","        predictions = tf.matmul(leaf_probs, self.leaf_scores)\n","        return predictions\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], 1)"]},{"cell_type":"code","execution_count":null,"id":"c0c6472e","metadata":{"id":"c0c6472e"},"outputs":[],"source":["def create_model(n_cuts, temperature, learning_rate):\n","    inputs = Input(shape=(X_train.shape[1],))\n","    doutputs = DNDTLayer(n_cuts=int(n_cuts), temperature=temperature)(inputs)\n","    model = Model(inputs=inputs, outputs=doutputs)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='mean_squared_error')\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"949b07e2","metadata":{"id":"949b07e2"},"outputs":[],"source":["def train_model(n_cuts, temperature, learning_rate):\n","    model = create_model(int(n_cuts), temperature, learning_rate)\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","    history = model.fit(\n","        X_train, Y_train,\n","        epochs=100,\n","        batch_size=32,\n","        validation_split=0.2,\n","        callbacks=[early_stopping],\n","        verbose=0\n","    )\n","    y_pred = model.predict(X_train).flatten()\n","    return -mean_squared_error(Y_train, y_pred) # -mse"]},{"cell_type":"code","execution_count":null,"id":"1ea98a17","metadata":{"id":"1ea98a17"},"outputs":[],"source":["# Define parameter bounds\n","pbounds = {'n_cuts': (1, 4),\n","           'temperature': (1.0, 20.0),\n","           'learning_rate': (0.001, 0.01)}\n","\n","# Perform Bayesian optimization\n","optimizer = BayesianOptimization(f=train_model, pbounds=pbounds, random_state=42)\n","optimizer.maximize(init_points=5, n_iter=10)\n","\n","# Get the best hyperparameters\n","best_params = optimizer.max['params']\n","\n","# Print the best hyperparameters\n","print(\"Best Hyperparameters:\")\n","print(best_params)"]},{"cell_type":"code","execution_count":null,"id":"98ce3e3d","metadata":{"id":"98ce3e3d"},"outputs":[],"source":["# Train the model with the best hyperparameters\n","best_model = create_model(**best_params)\n","best_model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n","\n","# Predict on the training set\n","y_pred_train = best_model.predict(X_train).flatten()\n","\n","# Calculate metrics on the training set\n","mse_train = mean_squared_error(Y_train, y_pred_train)\n","rmse_train = np.sqrt(mse_train)\n","mae_train = mean_absolute_error(Y_train, y_pred_train)\n","r2_train = 1 - mse_train / np.var(Y_train)\n","\n","# Print the results for training set\n","print('Training Set: ')\n","print(f'Mean Squared Error: {mse_train}')\n","print(f'Root Mean Squared Error: {rmse_train}')\n","print(f'Mean Absolute Error: {mae_train}')\n","print(f'R-squared: {r2_train}')"]},{"cell_type":"code","execution_count":null,"id":"c19ecc83","metadata":{"id":"c19ecc83"},"outputs":[],"source":["# Train the model with the best hyperparameters\n","best_model = create_model(**best_params)\n","best_model.fit(X_train, Y_train, epochs=100, batch_size=32, verbose=0)\n","\n","# Predict on the testing set\n","y_pred_test = best_model.predict(X_test).flatten()\n","\n","# Calculate metrics on the testing set\n","mse_test = mean_squared_error(Y_test, y_pred_test)\n","rmse_test = np.sqrt(mse_test)\n","mae_test = mean_absolute_error(Y_test, y_pred_test)\n","r2_test = 1 - mse_test / np.var(Y_test)\n","\n","# Print the results for testing set\n","print('\\nTesting Set: ')\n","print(f'Mean Squared Error: {mse_test}')\n","print(f'Root Mean Squared Error: {rmse_test}')\n","print(f'Mean Absolute Error: {mae_test}')\n","print(f'R-squared: {r2_test}')"]},{"cell_type":"code","execution_count":null,"id":"25196dd6","metadata":{"id":"25196dd6"},"outputs":[],"source":["# Predict on the training set\n","y_pred_train = best_model.predict(X_train).flatten()\n","\n","# Calculate residuals for training set\n","residuals_train = Y_train - y_pred_train\n","\n","# Predict on the testing set\n","y_pred_test = best_model.predict(X_test).flatten()\n","\n","# Calculate residuals for testing set\n","residuals_test = Y_test - y_pred_test"]},{"cell_type":"code","execution_count":null,"id":"469422e1","metadata":{"id":"469422e1"},"outputs":[],"source":["# Plot actual vs. predicted plot for both training and testing sets\n","plt.figure(figsize=(10, 6))\n","\n","# Plot training data\n","plt.scatter(Y_train, y_pred_train, color='blue', label='Training Data')\n","\n","# Plot testing data\n","plt.scatter(Y_test, y_pred_test, color='red', label='Testing Data')\n","\n","# Plot diagonal line\n","plt.plot([Y_train.min(), Y_train.max()], [Y_train.min(), Y_train.max()], color='black', lw=2, linestyle='--')\n","\n","plt.title('Actual vs. Predicted Plot')\n","plt.xlabel('Actual Values')\n","plt.ylabel('Predicted Values')\n","plt.legend()\n","plt.grid(True)\n","\n","# Save the figure with 600 DPI as a JPEG image\n","plt.savefig('F17.jpg', dpi=600, format='jpg', bbox_inches='tight')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"13c9fa24","metadata":{"id":"13c9fa24"},"outputs":[],"source":["# Plot residual plot for both training and testing sets\n","plt.figure(figsize=(10, 6))\n","\n","# Plot training residuals\n","plt.scatter(y_pred_train, residuals_train, color='blue', label='Training Data')\n","\n","# Plot testing residuals\n","plt.scatter(y_pred_test, residuals_test, color='red', label='Testing Data')\n","\n","# Plot horizontal line at y=0\n","plt.axhline(y=0, color='black', linestyle='-')\n","\n","plt.title('Residual Plot')\n","plt.xlabel('Predicted Values')\n","plt.ylabel('Residuals')\n","plt.legend()\n","plt.grid(True)\n","\n","# Save the figure with 600 DPI as a JPEG image\n","plt.savefig('F18.jpg', dpi=600, format='jpg', bbox_inches='tight')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"768e61c2","metadata":{"id":"768e61c2"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":5}